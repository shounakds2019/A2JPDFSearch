{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.7/site-packages (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz in /opt/anaconda3/lib/python3.7/site-packages\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=d76d92a5ff186a95328ce49a9229173d66bc5a2a08c55ce7eb1887d91cfb81bd\n",
      "  Stored in directory: /Users/shounakmondal/Library/Caches/pip/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
      "Successfully built en-core-web-sm\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyenchant\n",
      "  Downloading pyenchant-3.2.0-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 828 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyenchant\n",
      "Successfully installed pyenchant-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy english model (large)\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "import io\n",
    "\n",
    "# method for reading a pdf file\n",
    "def readPdfFile(filename, folder_name):\n",
    "    \n",
    "    resource_manager = PDFResourceManager()\n",
    "    fake_file_handle = io.StringIO()\n",
    "    converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "    with open('./docs/doc.pdf', 'rb') as fh:\n",
    "\n",
    "        for page in PDFPage.get_pages(fh,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "        text = fake_file_handle.getvalue()\n",
    "\n",
    "    # close open handles\n",
    "    converter.close()\n",
    "    fake_file_handle.close()\n",
    "\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer sentence segmenter for creating spacy document object\n",
    "def setCustomBoundaries(doc):\n",
    "    # traversing through tokens in document object\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == ';':\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "        if token.text == \".\":\n",
    "            doc[token.i + 1].is_sent_start = False\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create spacy document object from pdf text\n",
    "def getSpacyDocument(pdf_text, nlp):\n",
    "    main_doc = nlp(pdf_text)  # create spacy document object\n",
    "\n",
    "    return main_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding setCusotmeBoundaries to the pipeline\n",
    "nlp.add_pipe(setCustomBoundaries, before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# convert keywords to vector\n",
    "def createKeywordsVectors(keyword, nlp):\n",
    "    print(\"Searching document for word: \",keyword)\n",
    "    doc = nlp(keyword)  # convert to document object\n",
    "    #doc = Doc(nlp.vocab, words = keyword)  # convert to document object\n",
    "    #print(\"doc\",doc.vector)\n",
    "    return doc.vector\n",
    "\n",
    "\n",
    "# method to find cosine similarity\n",
    "def cosineSimilarity(vect1, vect2):\n",
    "    # return cosine distance\n",
    "    return 1 - spatial.distance.cosine(vect1, vect2)\n",
    "\n",
    "\n",
    "# method to find similar words\n",
    "def getSimilarWords(keyword, nlp):\n",
    "    similarity_list = []\n",
    "\n",
    "    keyword_vector = createKeywordsVectors(keyword, nlp)\n",
    "\n",
    "    for tokens in nlp.vocab:\n",
    "        if (tokens.has_vector):\n",
    "            if (tokens.is_lower):\n",
    "                if (tokens.is_alpha):\n",
    "                    similarity_list.append((tokens, cosineSimilarity(keyword_vector, tokens.vector)))\n",
    "\n",
    "    similarity_list = sorted(similarity_list, key=lambda item: -item[1])\n",
    "    similarity_list = similarity_list[:30]\n",
    "    top_similar_words = [item[0].text for item in similarity_list]\n",
    "    top_similar_words = top_similar_words[:3]\n",
    "    top_similar_words.append(keyword)\n",
    "    #print(\"top_similar_words:1\",top_similar_words)\n",
    "#     for token in Doc(nlp.vocab, words = keyword):\n",
    "#         top_similar_words.insert(0, token.lemma_)\n",
    "\n",
    "#     for words in top_similar_words:\n",
    "#         if words.endswith(\"s\"):\n",
    "#             top_similar_words.append(words[0:len(words)-1])\n",
    "\n",
    "#     top_similar_words = list(set(top_similar_words))\n",
    "    #print(\"top_similar_words:\",top_similar_words)\n",
    "    #top_similar_words = [words for words in top_similar_words if enchant_dict.check(words) == True]\n",
    "    top_similar_words = [words for words in top_similar_words if d.check(words) == True]\n",
    "    \n",
    "    top_similar_words = [words for words in top_similar_words]\n",
    "    similarity_list.clear()\n",
    "    return \", \".join(top_similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching document for word:  total cost\n",
      "Similar words are: total, cost, costs\n"
     ]
    }
   ],
   "source": [
    "#keywords = [\"safety\",\"guidelines\",\"manufacturers\"]\n",
    "keywords = \"total cost\"\n",
    "similar_keywords = getSimilarWords(keywords, nlp)\n",
    "print(\"Similar words are:\",similar_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from scipy import spatial\n",
    "\n",
    "# method for searching keyword from the text\n",
    "def search_for_keyword(keyword, doc_obj, nlp):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "    li = list(keyword.split(\",\")) \n",
    "    patterns = [nlp.make_doc(text) for text in li]\n",
    "    phrase_matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "    matched_items = phrase_matcher(doc_obj)\n",
    "    matched_text = []\n",
    "    for match_id, start, end in matched_items:\n",
    "        text = nlp.vocab.strings[match_id]\n",
    "        span = doc_obj[start: end]\n",
    "        matched_text.append(span.sent.text)\n",
    "    \n",
    "    for txt in matched_text:\n",
    "        print(\"Found Match ####\")\n",
    "        print(txt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = readPdfFile(\"testdoc.pdf\",\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_obj = getSpacyDocument(pdf,nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for... total, cost, costs \n",
      "\n",
      "Found Match ####\n",
      "Comprised of leading researchers investigating \n",
      "various  dimensions  of  access  to  justice  and  cost  across  the  country,  \n",
      "Found Match ####\n",
      "the  costs  of  justice,  \n",
      "Found Match ####\n",
      "the  monetary  costs  associated  with \n",
      "experiencing  everyday  legal  problems.  \n",
      "Found Match ####\n",
      "the  monetary  costs  to \n",
      "\n",
      "Found Match ####\n",
      "individuals  of  attempting  to  resolve  problems,  intangible  costs  associated  with  experiencing \n",
      "\n",
      "Found Match ####\n",
      "In sum, \n",
      "respondents  were  asked  about  costs  \n",
      "Found Match ####\n",
      "212  adults  using  cell  phones  (for  a  total  of  3,263  adult  Canadians).    \n",
      "Found Match ####\n",
      "the  individual  experiences  and  costs  of  everyday  \n",
      "Found Match ####\n",
      "addressing  the  costs  of  everyday  \n",
      "Found Match ####\n",
      "But  what  costs,  and  how  much?    \n",
      "Found Match ####\n",
      "These \n",
      "intangible  costs  can  include,  for  example,  decreasing  physical  health,  high \n",
      "levels of stress and emotional problems, and strains on relationships among \n",
      "family  members.    \n",
      "Found Match ####\n",
      "can  result  in  costs  to  \n",
      "Found Match ####\n",
      "the  total  cost  \n",
      "Found Match ####\n",
      "the  total  cost  \n",
      "Found Match ####\n",
      "almost 10% of the average total 2012 Canadian household expenditures \n",
      "\n",
      "($75,443); \n",
      "\n",
      "\n",
      "Found Match ####\n",
      "While a problem with a car may have required a total replacement in some \n",
      "instances,  to  avoid  potentially  overvaluing  our  spending  estimates,  outlier  responses  were \n",
      "excluded from our average spending figures.  \n",
      "Found Match ####\n",
      "This rounded total cost figure is based on weighted data. \n",
      " \n",
      "\n",
      "\n",
      "Found Match ####\n",
      "People are often required to travel \n",
      "for  appointments  or  appearances  at  courts  or  tribunals  –  and  the  cost,  for \n",
      "some,  can  be  considerable.36   \n",
      "Found Match ####\n",
      "the  cost  of  accessing  justice.  \n",
      "\n",
      "Found Match ####\n",
      "We  estimate  that  the  added  annual  costs  related  to  receiving  social \n",
      "assistance, a loss of employment, and physical and mental health issues – all \n",
      "as  \n",
      "Found Match ####\n",
      "legal  problems  – \n",
      "represent major annual costs to the state, amounting to a combined total of \n",
      "approximately $800 million (and perhaps significantly more).39  \n",
      "Found Match ####\n",
      "that  everyday  legal  problems  cost  the  state  \n",
      "Found Match ####\n",
      "that  everyday  legal  problems  cost  the  state  \n",
      "Found Match ####\n",
      ",  the  result  is  a  total  \n",
      "Found Match ####\n",
      "estimated  cost  of  $1.35  \n",
      "Found Match ####\n",
      "In total, we estimate that everyday legal problems cost the state $101 million \n",
      "annually  in  additional  health  \n",
      "Found Match ####\n",
      "related  costs  is  significantly  higher  than  our  original  –  very \n",
      "conservative  –  \n",
      "Found Match ####\n",
      "An \n",
      "average  cost  of  a  physician  \n"
     ]
    }
   ],
   "source": [
    "print(\"Searching for...\",similar_keywords,\"\\n\")\n",
    "search_for_keyword(similar_keywords,doc_obj, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
